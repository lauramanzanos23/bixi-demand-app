{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ec52b9",
   "metadata": {},
   "source": [
    "# Data Cleaning + EDA Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2262badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ importing all packages\n",
    "import requests, pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# ~ indicating where the datasets are stored\n",
    "\n",
    "#DATA_FOLDER = Path('C:/Users/calvi/INSY 662_Python/datasets')  #! replace with your correct path!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf16c8a",
   "metadata": {},
   "source": [
    "## BIXI Station Status Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c069f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='gbfs.velobixi.com', port=443): Max retries exceeded with url: /gbfs/2-2/en/station_information.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1513b0950>: Failed to resolve 'gbfs.velobixi.com' ([Errno 8] nodename nor servname provided, or not known)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    200\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    202\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    203\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:976\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    975\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[1;32m    977\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x1513b0950>: Failed to resolve 'gbfs.velobixi.com' ([Errno 8] nodename nor servname provided, or not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='gbfs.velobixi.com', port=443): Max retries exceeded with url: /gbfs/2-2/en/station_information.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1513b0950>: Failed to resolve 'gbfs.velobixi.com' ([Errno 8] nodename nor servname provided, or not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' This block of code will serve to import and read BIXI's station information dataset found as a json file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03min BIXI's official site looking at their GBSF feed '''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://gbfs.velobixi.com/gbfs/2-2/en/station_information.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m raw \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(URL, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#~ transforming the json file into a dataframe\u001b[39;00m\n\u001b[1;32m     10\u001b[0m stations_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstations\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='gbfs.velobixi.com', port=443): Max retries exceeded with url: /gbfs/2-2/en/station_information.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1513b0950>: Failed to resolve 'gbfs.velobixi.com' ([Errno 8] nodename nor servname provided, or not known)\"))"
     ]
    }
   ],
   "source": [
    "''' This block of code will serve to import and read BIXI's station information dataset found as a json file\n",
    "in BIXI's official site looking at their GBSF feed '''\n",
    "\n",
    "URL = \"https://gbfs.velobixi.com/gbfs/2-2/en/station_information.json\"\n",
    "\n",
    "raw = requests.get(URL, timeout=30).json()\n",
    "\n",
    "#~ transforming the json file into a dataframe\n",
    "\n",
    "stations_df = pd.json_normalize(raw[\"data\"][\"stations\"])\n",
    "\n",
    "#~ selecting all columns from the json file\n",
    "\n",
    "keep = [\n",
    "    \"station_id\", \"name\", \"short_name\", \"lat\", \"lon\", \"capacity\",\n",
    "    \"address\", \"post_code\", \"region_id\", \"external_id\",\n",
    "    \"rental_methods\"\n",
    "]\n",
    "stations_df = stations_df[[c for c in keep if c in stations_df.columns]].copy()\n",
    "\n",
    "\n",
    "#~ checks if rental_methods column is a list, if it is a list, \n",
    "#~ then join (e.g. [\"CREDITCARD\",\"APPLEPAY\"]), turn it into \"CREDITCARD,APPLEPAY\".)\n",
    "\n",
    "if \"rental_methods\" in stations_df.columns:\n",
    "    stations_df[\"rental_methods\"] = stations_df[\"rental_methods\"].apply(\n",
    "        lambda x: \",\".join(x) if isinstance(x, list) else x\n",
    "    )\n",
    "\n",
    "#~ viewing stations_df\n",
    "stations_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404c60d",
   "metadata": {},
   "source": [
    "### Understand the dataset - BIXI Station Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ see the numbers of rows and columns \n",
    "stations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154af2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~quick data type check and non-null count for stations_df\n",
    "\n",
    "stations_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aecd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ preview of dataframe head and tail\n",
    "\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe227f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ list of all columns names\n",
    "\n",
    "stations_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493c54e",
   "metadata": {},
   "source": [
    "\n",
    "### Data Quality Check (Data Cleaning Stage) - BIXI Station Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ quick data type changes and dropping unecessary column\n",
    "stations_df['station_id'] = stations_df['station_id'].astype(int)\n",
    "stations_df['short_name'] = stations_df['short_name'].astype(int)\n",
    "\n",
    "stations_df = stations_df.drop(columns=['rental_methods','external_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb50519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ checking updated dataframe\n",
    "\n",
    "''' it seems there is no null value (yay!) '''\n",
    "\n",
    "stations_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ checking if duplicated rows exist\n",
    "\n",
    "stations_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43340dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~checking if there are duplicates in each columns\n",
    "\n",
    "\"\"\"the duplicates for latitude, longtitude, and capacity make since!   \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "column_duplicate_check = stations_df.apply(lambda s: s.nunique(dropna=False)< len(s))\n",
    "\n",
    "print(column_duplicate_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ the dataframe below is the cleaned stations_df\n",
    "\n",
    "clean_stations_df = stations_df.copy()\n",
    "\n",
    "clean_stations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d511c1",
   "metadata": {},
   "source": [
    "### Summary statistics (High-Level profile) - BIXI Station Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b64a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ high level statistic of all columns\n",
    "\n",
    "clean_stations_df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ frequency of values for capacity\n",
    "\n",
    "clean_stations_df['capacity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ad703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ proportion of each values within capacity column\n",
    "\n",
    "clean_stations_df['capacity'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467d818",
   "metadata": {},
   "source": [
    "### Quick Univariate Analysis - BIXI Station Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ capacity seems to be interesting\n",
    "\n",
    "sns.boxenplot(x=clean_stations_df['capacity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ verifying if certain stations are outliers --> it seems they are very niche cases! Only station #724 that has 3 capacity is weird.\n",
    "\n",
    "filter_station = (clean_stations_df[\"capacity\"] > 40) | (clean_stations_df[\"capacity\"] < 10)\n",
    "clean_stations_df.loc[filter_station]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6210c990",
   "metadata": {},
   "source": [
    "## BIXI 2024 Trip History Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82373be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ importing and reading BIXI 2024 Trip history dataset (OVER 13M rows of data!!)\n",
    "#~ data source: https://bixi.com/en/open-data/   choose the 2024\n",
    "\n",
    "bixi_trip_df =  pd.read_csv('BIXI_Trip_2024.csv')   \n",
    "# bixi_trip_df =  pd.read_csv('BIXI_Trip_2024.csv')   \n",
    "\n",
    "#! I changed the downloaded csv to this name, make sure to reflect if not the same name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1245f5",
   "metadata": {},
   "source": [
    "### Understand the dataset - BIXI 2024 Trip History Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ looking at shape\n",
    "\n",
    "bixi_trip_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952415a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ checking data types \n",
    "bixi_trip_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ head and tail of bixi trip dataframe\n",
    "\n",
    "bixi_trip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93568b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bixi_trip_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8709f5",
   "metadata": {},
   "source": [
    "\n",
    "### Data Quality Check - BIXI 2024 Trip History Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ standardizing the columnNames\n",
    "bixi_trip_df.columns = (bixi_trip_df.columns              \n",
    "              .str.normalize('NFKD')                    \n",
    "              .str.encode('ascii','ignore').str.decode('ascii')\n",
    "              .str.strip()\n",
    "              .str.lower()\n",
    "              .str.replace(r'[^a-z0-9]+', '_', regex=True)  \n",
    "              .str.replace(r'_+', '_', regex=True)          \n",
    "              .str.strip('_'))\n",
    "\n",
    "bixi_trip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ checking for null values\n",
    "\n",
    "\n",
    "bixi_trip_df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c130335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#~ it seems that a majority of the null values are missing the endstation name and therefore has no endtime. We will assume that this is error in data capture\n",
    "#~ dropping all null values\n",
    "\n",
    "bixi_trip_df= bixi_trip_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ double checking to make sure\n",
    "bixi_trip_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ checking how many rows left after dropping nulls :  went from 13,275,326  to 13,199,617   dropped exactly 75,709 rows!!\n",
    "bixi_trip_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ creating a new column by transforming the startimems and endtimems  (these columns are in UNIX format)\n",
    "#~ into datetime formatted as YYYY-MM-DD HH-mm-ss\n",
    "\n",
    "bixi_trip_df['starttime'] = (\n",
    "    pd.to_datetime(bixi_trip_df[\"starttimems\"], unit='ms', utc=True)\n",
    "      .dt.floor(\"s\")                           #~ the data showed time zone in ISO-8601 format, I stopped at seconds\n",
    "      .dt.tz_convert(\"America/Toronto\")\n",
    "      .dt.tz_localize(None)        \n",
    ")\n",
    "\n",
    "\n",
    "bixi_trip_df['endtime'] = (\n",
    "    pd.to_datetime(bixi_trip_df[\"endtimems\"], unit='ms', utc=True)\n",
    "      .dt.floor(\"s\")                           \n",
    "      .dt.tz_convert(\"America/Toronto\")\n",
    "      .dt.tz_localize(None)       \n",
    ")\n",
    "\n",
    "bixi_trip_df = bixi_trip_df.drop(columns=['starttimems', 'endtimems']) #~ dropping the initial unix time columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222eb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ there are duplicated rows but this is normal since each row in this dataset represent trip data\n",
    "bixi_trip_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ checking the dtypes and updating clean df as clean\n",
    "\n",
    "bixi_trip_df.info()\n",
    "\n",
    "clean_bixi_trip_df = bixi_trip_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81354c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ adding column to bring out the hour of the day, the day of week, day of month, and month of the year for starting time and endtime\n",
    "\n",
    "#~Hour of day (0–23)\n",
    "\n",
    "clean_bixi_trip_df[\"start_hour\"] = clean_bixi_trip_df[\"starttime\"].dt.hour\n",
    "clean_bixi_trip_df[\"end_hour\"]   = clean_bixi_trip_df[\"endtime\"].dt.hour\n",
    "\n",
    "#~Day of week (1–7, Monday=1, Sunday=7)\n",
    "\n",
    "clean_bixi_trip_df[\"start_day\"] = clean_bixi_trip_df[\"starttime\"].dt.dayofweek + 1\n",
    "clean_bixi_trip_df[\"end_day\"]   = clean_bixi_trip_df[\"endtime\"].dt.dayofweek + 1\n",
    "\n",
    "#~Day of month (1–31)\n",
    "clean_bixi_trip_df[\"start_dayofmonth\"] = clean_bixi_trip_df[\"starttime\"].dt.day\n",
    "clean_bixi_trip_df[\"end_dayofmonth\"]   = clean_bixi_trip_df[\"endtime\"].dt.day\n",
    "\n",
    "#~Month of year (1–12)\n",
    "\n",
    "clean_bixi_trip_df[\"start_month\"] = clean_bixi_trip_df[\"starttime\"].dt.month\n",
    "clean_bixi_trip_df[\"end_month\"]   = clean_bixi_trip_df[\"endtime\"].dt.month\n",
    "\n",
    "clean_bixi_trip_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba270e5",
   "metadata": {},
   "source": [
    "### Quick Univariate Analysis - BIXI 2024 Trip History Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ counts of top 10 starting and ending stations\n",
    "\n",
    "top10_start_count = clean_bixi_trip_df['startstationname'].value_counts().head(10).sort_values()\n",
    "\n",
    "top10_start_count.plot(kind='barh', xlabel='Count', ylabel=clean_bixi_trip_df['startstationname'], title='Top 10 starting station')\n",
    "plt.tight_layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ counts of top 10 ending stations\n",
    "\n",
    "top10_end_count = clean_bixi_trip_df['endstationname'].value_counts().head(10).sort_values()\n",
    "\n",
    "top10_end_count.plot(kind='barh', xlabel='Count', ylabel=clean_bixi_trip_df['endstationname'], title='Top 10 ending station')\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c05518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ counts of top 10 starting boroughs\n",
    "\n",
    "top10_startBor_count = clean_bixi_trip_df['startstationarrondissement'].value_counts().head(10).sort_values()\n",
    "\n",
    "top10_startBor_count.plot(kind='barh', xlabel='Count', ylabel=clean_bixi_trip_df['startstationarrondissement'], title='Top 10 starting boroughs')\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ counts of top 10 ending boroughs\n",
    "\n",
    "top10_endBor_count = clean_bixi_trip_df['endstationarrondissement'].value_counts().head(10).sort_values()\n",
    "\n",
    "top10_endBor_count.plot(kind='barh', xlabel='Count', ylabel=clean_bixi_trip_df['endstationarrondissement'], title='Top 10 ending boroughs')\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#~ bar chart to discover most popular day of week of trips for starting station\n",
    "\n",
    "daily_counts_start = (clean_bixi_trip_df['start_day'].value_counts().sort_index())\n",
    "\n",
    "ax = daily_counts_start.plot.bar(figsize=(12,4), rot=90)\n",
    "ax.set_xlabel(\"Date of week\")\n",
    "ax.set_ylabel(\"Trips\")\n",
    "ax.set_title(\"Trips per day of week for starting stations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b19c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ bar chart to discover most popular day of week of trips for ending station\n",
    "\n",
    "daily_counts_end = (clean_bixi_trip_df['end_day'].value_counts().sort_index())\n",
    "\n",
    "ax = daily_counts_end.plot.bar(figsize=(12,4), rot=90)\n",
    "ax.set_xlabel(\"Date of week\")\n",
    "ax.set_ylabel(\"Trips\")\n",
    "ax.set_title(\"Trips per day for ending station\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b56a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ looking at the trips for each hour for starting station\n",
    "\n",
    "hourly_counts_start = (clean_bixi_trip_df['start_hour'].value_counts().sort_index())\n",
    "\n",
    "ax = hourly_counts_start.plot.bar(figsize=(12,4), rot=90)\n",
    "ax.set_xlabel(\"Hour of day\")\n",
    "ax.set_ylabel(\"Trips\")\n",
    "ax.set_title(\"Trips per hour of day for starting station\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ looking at the trips for each hour for starting station\n",
    "\n",
    "hourly_counts_end = (clean_bixi_trip_df['end_hour'].value_counts().sort_index())\n",
    "\n",
    "ax = hourly_counts_end.plot.bar(figsize=(12,4), rot=90)\n",
    "ax.set_xlabel(\"Hour of day\")\n",
    "ax.set_ylabel(\"Trips\")\n",
    "ax.set_title(\"Trips per hour of day for ending station\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402fca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ bar chart to discover most popular day of month of trips for starting station\n",
    "\n",
    "daily_counts_startday = (clean_bixi_trip_df['start_dayofmonth'].value_counts().sort_index())\n",
    "\n",
    "ax = daily_counts_startday.plot.bar(figsize=(12,4), rot=90)\n",
    "ax.set_xlabel(\"Day of Month\")\n",
    "ax.set_ylabel(\"Trips\")\n",
    "ax.set_title(\"Trips per day of month for starting stations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ bar chart to discover most popular  month of trips for starting station\n",
    "\n",
    "daily_counts_startmonth = (clean_bixi_trip_df['start_month'].value_counts().sort_index())\n",
    "\n",
    "ax = daily_counts_startmonth.plot.bar(figsize=(12,4), rot=90)\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Trips\")\n",
    "ax.set_title(\"Trips per month for starting stations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ filterting the bixi trip dataset to only have month 5,6,7,8, 9 and 10 \n",
    "\n",
    "clean_bixi_trip_df.query('5 <= start_month <= 10', inplace=True)\n",
    "\n",
    "clean_bixi_trip_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa808da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bixi_trip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25412c20",
   "metadata": {},
   "source": [
    "## Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e47d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ importing weather data set  source: https://montreal.weatherstats.ca/download.html \n",
    "#~ choose 'Climate Hourly' option and then press download\n",
    "\n",
    "weather_df =  pd.read_csv('mtl_weather_2024.csv')  #! make sure your file name is same!\n",
    "# weather_df =  pd.read_csv('mtl_weather_2024.csv') \n",
    "#~ checking top 5 rows of dataframe\n",
    "weather_df.head() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ # Check the datatype of all columns\n",
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert unixtime to datetime (localized to America/Toronto time zone)\n",
    "weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"unixtime\"], unit=\"s\", utc=True).dt.tz_convert(\"America/Toronto\").dt.tz_localize(None)\n",
    "\n",
    "# Create three new columns\n",
    "weather_df[\"hour\"] = weather_df[\"datetime\"].dt.hour                  # hour of the day (0–23)\n",
    "weather_df[\"day_of_week\"] = weather_df[\"datetime\"].dt.dayofweek + 1  # day of week (1–7, Monday=1)\n",
    "weather_df[\"month\"] = weather_df[\"datetime\"].dt.month                # month of the year (1–12)\n",
    "weather_df[\"year\"] = weather_df['datetime'].dt.year                  # year\n",
    "# Keep only data from May (5) to October (10)\n",
    "updated_weather_df = weather_df[weather_df[\"month\"].between(5, 10)]\n",
    "\n",
    "#~ filtering only 2024\n",
    "updated_weather_df = updated_weather_df.query('year == 2024').reset_index(drop=True)\n",
    "\n",
    "updated_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick sanity check\n",
    "updated_weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e85276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Based on the missing value profiles and each variable’s potential impact on BIXI trips, \n",
    "we select temperature, wind_speed, relative_humidity, and visibility as the weather variables likely to affect ridership. \n",
    "And we treat conditions with high relative_humidity and low visibility as bad weather (e.g., rain).\n",
    "Together with the newly created time variables, we build a new dataframe: updated_weather_df.\n",
    "'''\n",
    "weather_column_keep = ['datetime', 'hour', 'day_of_week', 'month',\n",
    "                       'temperature', 'wind_speed', 'relative_humidity', 'visibility']    \n",
    "to_use_weather_cols = [c for c in weather_column_keep if c in updated_weather_df.columns]\n",
    "updated_weather_df = updated_weather_df[to_use_weather_cols].copy()\n",
    "\n",
    "updated_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01db200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check dtype of columns\n",
    "# The data types seem to be fine as a baseline\n",
    "updated_weather_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1befc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "updated_weather_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa111b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill in the missing values of visibility. Since visibility usually changes smoothly over time, linear interpolation is a suitable method\n",
    "# Interpolate visibility linearly based on time\n",
    "updated_weather_df[\"visibility\"] = updated_weather_df[\"visibility\"].interpolate(method=\"linear\")\n",
    "\n",
    "# If still any missing (e.g. at start/end of data), fill with nearest valid value\n",
    "updated_weather_df[\"visibility\"] = (\n",
    "    updated_weather_df[\"visibility\"].bfill().ffill())\n",
    "\n",
    "updated_weather_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~  Distribution analysis of weather variables\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weather_cols = [\"temperature\", \"wind_speed\", \"relative_humidity\", \"visibility\"]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(weather_cols, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    updated_weather_df[col].hist(bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ea7d0",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation:**\n",
    "From the four histograms above, we can observe that the temperature distribution is approximately normal, with a reasonable range and no apparent outliers. \n",
    "\n",
    "Wind speed shows a right-skewed distribution, with a few instances of high values but still within a reasonable range. \n",
    "\n",
    "Relative humidity is mostly concentrated between 60% and 90%, with no outliers, indicating that Montreal generally experiences high humidity during summer and autumn, which suggests that BIXI ridership may be notably affected by rainy conditions.\n",
    "\n",
    "For visibility, most values are concentrated around 24100 m, suggesting this is the upper limit of the measurement system. A few values near 50000 m far exceed realistic urban atmospheric visibility and are considered outliers. To address this, we categorized visibility into bins.\n",
    "\n",
    "Based on meteorological conventions, when visibility is below 5000 m, conditions such as fog or rain are likely to cause a significant decline in ridership; when visibility is above 15000 m, weather conditions are generally stable and have minimal impact on travel. Therefore, visibility is divided into three categories — low, medium, and high — corresponding to the ranges 0–5000 m, 5000–15000 m, and above 15000 m, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ Bin the visibility variable\n",
    "#~ Define bins and labels\n",
    "bins = [0, 5000, 15000, float('inf')] \n",
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "#~ Create a new categorical column for visibility levels\n",
    "updated_weather_df['visibility_level'] = pd.cut(\n",
    "    updated_weather_df['visibility'],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    include_lowest=True,\n",
    "    right=False \n",
    ")\n",
    "\n",
    "# Check the distribution\n",
    "print(updated_weather_df['visibility_level'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the average relative humidity for each visibility category\n",
    "grouped = updated_weather_df.groupby('visibility_level')['relative_humidity'].mean().reset_index()\n",
    "\n",
    "# Visualize\n",
    "sns.barplot(data=grouped, x='visibility_level', y='relative_humidity', color='skyblue')\n",
    "plt.title('Average Relative Humidity by Visibility Level')\n",
    "plt.xlabel('Visibility Level')\n",
    "plt.ylabel('Average Relative Humidity (%)')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Spearman rank correlation between relative humidity and visibility level\n",
    "corr = updated_weather_df['relative_humidity'].corr(updated_weather_df['visibility_level'].cat.codes, method='spearman')\n",
    "print(\"Spearman correlation:\", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc5544",
   "metadata": {},
   "source": [
    "**Interpretation:**  As visibility decreases, average relative humidity rises sharply. Low-visibility periods correspond to humidity above 90%, while clear conditions show about 70%. The negative Spearman correlation (-0.315) confirms that high humidity is typically associated with reduced visibility, indicating foggy or rainy weather. This supports creating a bad_weather variable combining humidity and visibility to capture adverse cycling conditions.\n",
    "\n",
    "Based on meteorological conventions, we define bad_weather as cases where relative_humidity > 85% and visibility < 10000 m. A humidity level above 85% generally indicates very moist conditions and often precedes visible precipitation, while visibility below 10,000 meters typically indicates foggy or rainy weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e927ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new variable bad_weather\n",
    "updated_weather_df[\"bad_weather\"] = (\n",
    "    (updated_weather_df[\"relative_humidity\"] > 85) &\n",
    "    (updated_weather_df[\"visibility\"] < 10000)\n",
    ").astype(int)\n",
    "\n",
    "# Check the distribution of bad vs. good weather\n",
    "print(updated_weather_df[\"bad_weather\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ac269",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Considering that temperature and wind_speed may require standardization for certain models, we create two new columns to store their standardized values. \n",
    "At the same time, the original columns are retained in the final dataframe to allow future exploration with models that do not require standardization.\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols_to_scale = ['temperature', 'wind_speed']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "scaled_values = scaler.fit_transform(updated_weather_df[cols_to_scale])\n",
    "\n",
    "# Create new standardized columns\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    updated_weather_df[f\"{col}_scaled\"] = scaled_values[:, i]\n",
    "\n",
    "updated_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose and reorder the columns of updated_weather_df and reset the index to create the final cleaned_weather_df\n",
    "cleaned_weather_df = updated_weather_df[['datetime', 'hour', 'day_of_week', 'month',\n",
    "                                        'temperature', 'temperature_scaled', 'wind_speed', 'wind_speed_scaled', 'bad_weather']]\n",
    "cleaned_weather_df = cleaned_weather_df.reset_index(drop=True)\n",
    "cleaned_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537824da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb08a142",
   "metadata": {},
   "source": [
    "# Feature Engineering and EDA cycle (back and forth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713a59a",
   "metadata": {},
   "source": [
    "<h5 style='color:Orange'> As of this instance, <br>\n",
    "\n",
    "*<strong> the 3 variable for our clean dataframe of our datasets are the following:* </strong> <br>\n",
    "\n",
    "\n",
    "<strong> clean_stations_df </strong>  : the station info (capacity, name, id, etc.)\n",
    "\n",
    "<strong> clean_bixi_trip_df </strong> : the bixi historical data of 2024 data\n",
    "\n",
    "<strong> cleaned_weather_df </strong>: montreal 2024 weather data per day \n",
    "\n",
    "\n",
    "\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f47b5",
   "metadata": {},
   "source": [
    "## Feature Engineering for Bixi_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bixi_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf689bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ to find the duration of each trip. \n",
    "\n",
    "clean_bixi_trip_df['trip_duration'] = clean_bixi_trip_df['endtime'] - clean_bixi_trip_df['starttime']\n",
    "\n",
    "clean_bixi_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e969b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ converting the trip_duration in minutes\n",
    "\n",
    "td = pd.to_timedelta(clean_bixi_trip_df['trip_duration'], errors='coerce')\n",
    "\n",
    "\n",
    "#~ rounding the duration to the nearest minute\n",
    "clean_bixi_trip_df['trip_duration_min'] = td.dt.total_seconds() / 60\n",
    "clean_bixi_trip_df['trip_duration_min_round'] = clean_bixi_trip_df['trip_duration_min'].round(2)\n",
    "\n",
    "#~ dropping columns that won't be useful for our model\n",
    "clean_bixi_trip_df = clean_bixi_trip_df.drop(columns=['trip_duration_min','startstationlatitude','startstationlongitude','endstationlatitude','endstationlongitude'])\n",
    "\n",
    "clean_bixi_trip_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683df5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ having a quick descriptive statistic for our trip duration \n",
    "with pd.option_context('display.float_format', '{:,.2f}'.format):\n",
    "    print(clean_bixi_trip_df['trip_duration_min_round'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ it seems like there are trip duration <= 0 min, therefore dropping all of them\n",
    "#~ in this instance we keep only tripduration > 0 \n",
    "\n",
    "clean_bixi_trip_df = (clean_bixi_trip_df[clean_bixi_trip_df['trip_duration_min_round'] > 0].reset_index(drop=True))\n",
    "\n",
    "clean_bixi_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86048c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ Looking at the descriptive stats of the column, it seems there are outliers\n",
    "#~ building a boxplot to see how severe it is. \n",
    "\n",
    "s = pd.to_numeric(clean_bixi_trip_df[\"trip_duration_min_round\"], errors=\"coerce\").dropna()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure()\n",
    "sns.boxplot(x=s)\n",
    "plt.xlabel(\"Trip duration (minutes)\")\n",
    "plt.title(\"BIXI Trip Duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"Using Z-Score to identify outliers\"\"\"\n",
    "\n",
    "col = \"trip_duration_min_round\"\n",
    "\n",
    "s = pd.to_numeric(clean_bixi_trip_df[col], errors=\"coerce\")\n",
    "\n",
    "#~ compute Z-score\n",
    "mu, sigma = s.mean(skipna=True), s.std(skipna=True, ddof=0)\n",
    "z = (s - mu) / sigma\n",
    "\n",
    "#~ creating a column to flag if it is a outlier Z > 3\n",
    "thr = 3.0\n",
    "clean_bixi_trip_df[\"z_trip_min\"] = z\n",
    "clean_bixi_trip_df[\"is_outlier_z3\"] = z.abs() > thr\n",
    "\n",
    "#~quick summary\n",
    "n = s.notna().sum()\n",
    "n_out = clean_bixi_trip_df[\"is_outlier_z3\"].sum()\n",
    "print(f\"Outliers (|z|>{thr}): {n_out} of {n} = {n_out/n:.2%}\")\n",
    "\n",
    "#~ to view outliers\n",
    "outliers = clean_bixi_trip_df.loc[clean_bixi_trip_df[\"is_outlier_z3\"]].sort_values(col, ascending=False)\n",
    "\n",
    "outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ dropping all rows that have been tagged as outliers (keep only non-outliers)\n",
    "\n",
    "clean_bixi_trip_df = (\n",
    "    clean_bixi_trip_df.loc[~clean_bixi_trip_df[\"is_outlier_z3\"]]\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ quick check of remaining data,  it seems there are still outliers.. 1000 min of trip duration...\n",
    "\n",
    "with pd.option_context('display.float_format', '{:,.2f}'.format):\n",
    "    print(clean_bixi_trip_df['trip_duration_min_round'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' After a bit of research, BIXI charges 18 cents after 45 minutes of the initial use. This encourage people to dock their bikes\n",
    "for a break to restart the 45-minutes free period. With this in mind, we put a hard cutoff for any trip duration that is more than 120 min  '''\n",
    "\n",
    "clean_bixi_trip_df = (\n",
    "    clean_bixi_trip_df[clean_bixi_trip_df[\"trip_duration_min_round\"] <= 120]\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ef4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ dropping the outlier column tag created earlier\n",
    "clean_bixi_trip_df = clean_bixi_trip_df.drop(columns=['is_outlier_z3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ab041",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = s[(s > 0) & (s <= 120)].dropna()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure()\n",
    "sns.histplot(sub, bins=range(0, 121, 5), edgecolor=\"white\")  \n",
    "plt.xlim(0, 120)\n",
    "plt.xlabel(\"Trip duration (minutes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"BIXI Trip Duration (<= 120 min)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83961ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''  The dataset for bixi trip is now updated to have duration'''\n",
    "clean_bixi_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f4b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10efdb0a",
   "metadata": {},
   "source": [
    "\n",
    "# Towards the final dataset for modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344453f",
   "metadata": {},
   "source": [
    "<h5>\n",
    "\n",
    "clean data set names: \n",
    "\n",
    "clean_stations_df  ===> station dataset\n",
    "\n",
    "clean_bixi_trip_df ===> bixi 2024 trip history\n",
    "\n",
    "cleaned_weather_df ===> hourly weather dataset from 2024\n",
    "\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bixi_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3747d3c",
   "metadata": {},
   "source": [
    "We create the final dataset ready for modeling model_df by filtering the top400 stations with more demand overall and grouping those stations by demand per hour (sum of start trip + end trip). Then merge with weather dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  hourly rows for STARTS (departures) and ENDS (arrivals)\n",
    "starts = (\n",
    "    clean_bixi_trip_df\n",
    "      .loc[:, ['startstationname', 'startstationarrondissement', 'starttime']]\n",
    "      .rename(columns={\n",
    "          'startstationname': 'station',\n",
    "          'startstationarrondissement': 'arrondissement',\n",
    "          'starttime': 'datetime'\n",
    "      })\n",
    ")\n",
    "ends = (\n",
    "    clean_bixi_trip_df\n",
    "      .loc[:, ['endstationname', 'endstationarrondissement', 'endtime']]\n",
    "      .rename(columns={\n",
    "          'endstationname': 'station',\n",
    "          'endstationarrondissement': 'arrondissement',\n",
    "          'endtime': 'datetime'\n",
    "      })\n",
    ")\n",
    "\n",
    "# Hourly buckets (so starts/ends land in their own hour)\n",
    "starts['datetime'] = starts['datetime'].dt.floor('h')\n",
    "ends['datetime']   = ends['datetime'].dt.floor('h')\n",
    "\n",
    "#  concatenate starts and ends and count each row = 1 unit of demand\n",
    "long_df = pd.concat([starts, ends], ignore_index=True)\n",
    "long_df['ones'] = 1\n",
    "\n",
    "# keep only the TOP 400 stations by total demand OVERALL\n",
    "# total demand per station (starts + ends) from the long table\n",
    "station_totals = (\n",
    "    long_df.groupby('station', as_index=False)['ones']\n",
    "           .sum()\n",
    "           .sort_values('ones', ascending=False)\n",
    ")\n",
    "\n",
    "top_400 = station_totals.head(400)['station']\n",
    "\n",
    "# Filter long_df to just those stations (TOP 400)\n",
    "long_df = long_df[long_df['station'].isin(top_400)].copy()\n",
    "\n",
    "\n",
    "# Hourly demand per station + arrondissement ( both starts & ends combined)\n",
    "hourly_station_demand = (\n",
    "    long_df\n",
    "      .groupby(['station', 'arrondissement', 'datetime'], as_index=False)['ones']\n",
    "      .sum()\n",
    "      .rename(columns={'ones': 'total_demand'})\n",
    ")\n",
    "\n",
    "hourly_station_demand['hour']  = hourly_station_demand['datetime'].dt.hour\n",
    "hourly_station_demand['day']   = hourly_station_demand['datetime'].dt.day\n",
    "hourly_station_demand['month'] = hourly_station_demand['datetime'].dt.month\n",
    "hourly_station_demand['year']  = hourly_station_demand['datetime'].dt.year\n",
    "\n",
    "# Merge with weather (per hour) \n",
    "weather_cols = ['datetime', 'temperature', 'wind_speed', 'bad_weather',\n",
    "                'temperature_scaled', 'wind_speed_scaled', 'day_of_week', 'month', 'hour']\n",
    "weather = cleaned_weather_df.loc[:, [c for c in weather_cols if c in cleaned_weather_df.columns]]\n",
    "\n",
    "model_df = hourly_station_demand.merge(\n",
    "    weather, on='datetime', how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1ae99",
   "metadata": {},
   "source": [
    "From the weather data there's some values from November 1, that we don't have in the bixi_trips dataset, we remove those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af025e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df[model_df['temperature'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74239f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NAN values, from dates that aren't on Bixi_trips (november 1)\n",
    "model_df = model_df.dropna(subset=['temperature', 'wind_speed', 'temperature_scaled', 'wind_speed_scaled'])\n",
    "model_df = model_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c43c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate columns and rename\n",
    "model_df = (\n",
    "    model_df\n",
    "      .drop(columns=['hour_y','month_y'], errors='ignore')\n",
    "      .rename(columns={'hour_x':'hour','month_x':'month'})\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093022af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move total_demand to the end of the dataframe\n",
    "cols = [c for c in model_df.columns if c != 'total_demand'] + ['total_demand']\n",
    "model_df = model_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da206ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL DATASET READY FOR MODELING\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ adding new features =====================================================\n",
    "\n",
    "#~ is weekend?\n",
    "model_df['is_weekend'] = model_df['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# #~ cycle time loop \n",
    "model_df['hour_sin'] = np.sin(2*np.pi*model_df['hour']/24)\n",
    "model_df['hour_cos'] = np.cos(2*np.pi*model_df['hour']/24)\n",
    "\n",
    "\n",
    "\n",
    "#~ feels like weather\n",
    "\n",
    "model_df['feels_like'] = model_df['temperature'] - 0.7 * model_df['wind_speed']\n",
    "\n",
    "\n",
    "# # #~ borought encoded:\n",
    "\n",
    "# model_df['arrondissement_encoded'] = model_df['arrondissement'].astype('category').cat.codes\n",
    "\n",
    "#~ avg_hourly_demand_station\n",
    "model_df = model_df.sort_values(['station','datetime'])\n",
    "\n",
    "model_df['avg_hourly_demand_station'] = (\n",
    "    model_df.groupby(['station','hour'])['total_demand']\n",
    "      .apply(lambda s: s.shift(1).expanding(min_periods=1).mean())\n",
    "      .reset_index(level=[0,1], drop=True)\n",
    ")\n",
    "#~ average day of week station\n",
    "\n",
    "avg_dow_hist = (\n",
    "    model_df.groupby(['station','day_of_week'])['total_demand']\n",
    "            .apply(lambda s: s.shift(1).expanding(min_periods=1).mean())\n",
    "            .reset_index(level=[0,1], drop=True)\n",
    ")\n",
    "\n",
    "model_df['avg_dayofweek_station'] = avg_dow_hist\n",
    "\n",
    "#~ for stations with no historical \n",
    "station_past_mean = (\n",
    "    model_df.groupby('station')['total_demand']\n",
    "            .apply(lambda s: s.shift(1).expanding(min_periods=1).mean())\n",
    "            .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "for col in ['avg_hourly_demand_station', 'avg_dayofweek_station']:\n",
    "    model_df[col] = model_df[col].fillna(station_past_mean)\n",
    "\n",
    "#~ fill station without historical data\n",
    "model_df['avg_dayofweek_station'] = model_df['avg_dayofweek_station'].fillna(0)\n",
    "\n",
    "#~ fill hourly demand station without historical data\n",
    "model_df['avg_hourly_demand_station'] = model_df['avg_hourly_demand_station'].fillna(0)\n",
    "\n",
    "\n",
    "#~ adding holiday Quebec/Montreal public holidays \n",
    "holidays_2024 = [\n",
    "    \"2024-01-01\",  # New Year's Day\n",
    "    \"2024-03-29\",  # Good Friday\n",
    "    \"2024-04-01\",  # Easter Monday (public sector)\n",
    "    \"2024-05-20\",  # National Patriots' Day\n",
    "    \"2024-06-24\",  # Fête nationale du Québec\n",
    "    \"2024-07-01\",  # Canada Day\n",
    "    \"2024-09-02\",  # Labour Day\n",
    "    \"2024-10-14\",  # Thanksgiving\n",
    "    \"2024-12-25\",  # Christmas\n",
    "    \"2024-12-26\",  # Boxing Day\n",
    "]\n",
    "\n",
    "holidays_2024 = pd.to_datetime(holidays_2024)\n",
    "\n",
    "#~ adding if holiday in df \n",
    "\n",
    "model_df['is_holiday'] = model_df['datetime'].dt.date.isin(holidays_2024.date).astype(int)\n",
    "\n",
    "#~ weather interaction terms (temperature x hour)\n",
    "\n",
    "model_df['temp_hour']= model_df['temperature_scaled']*model_df['hour']\n",
    "\n",
    "#~ curved comfort pattern of temperature\n",
    "\n",
    "model_df['temperature_sq'] = model_df['temperature_scaled']**2 \n",
    "\n",
    "#~ temperature x feels_like interaction term: \n",
    "\n",
    "model_df['temp_feels_interaction'] = model_df['temperature_scaled'] * model_df['feels_like']\n",
    "\n",
    "#~ simplyifing daily patterns night, morning, day, evening, late\n",
    "\n",
    "model_df['hour_bucket'] = pd.cut(model_df['hour'],\n",
    "                           bins=[-1,5,10,16,19,24],\n",
    "                           labels=['night','morning','day','evening','late'],\n",
    "                           include_lowest=True).astype('category').cat.codes\n",
    "\n",
    "#~ weekend hour interaction \n",
    "\n",
    "model_df['weekend_hour_interaction'] = model_df['is_weekend'] * model_df['hour_sin']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ creating a small dataframe for joining the station names to lat and lon\n",
    "coords_df = (\n",
    "    bixi_trip_df[\n",
    "        ['startstationname', 'startstationlatitude', 'startstationlongitude']\n",
    "    ]\n",
    "    .drop_duplicates(subset='startstationname')  # keep 1 row per station\n",
    "    .rename(columns={\n",
    "        'startstationname': 'station',\n",
    "        'startstationlatitude': 'lat',\n",
    "        'startstationlongitude': 'lon'\n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.merge(\n",
    "    coords_df[['station', 'lat', 'lon']],\n",
    "    on='station',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#~ bixi_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee477cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6271011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~ exporting csv \n",
    "\n",
    "model_df.to_csv(\"BIXI_MODEL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e72b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
